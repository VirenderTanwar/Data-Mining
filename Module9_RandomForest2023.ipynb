{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9GyaufTuK6F"
   },
   "source": [
    "#**Module 9: Random Forest Classification**\n",
    "\n",
    "In this notebook, we are going to set up a Random Forest model in Python. At the end of this module, you will be able to:\n",
    "\n",
    "* Explain what Random Forest does\n",
    "* Build and evaluate a Random Forest\n",
    "\n",
    "**Be sure to expand all the hidden cells, run all the code, and do all the exercises--you will need the techniques for the lesson lab!**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypv23z_l-Vx7"
   },
   "source": [
    "#**What is Random Forest?**\n",
    "For the previous module on simple tree construction, there's a chance that each of you have slightly different numeric results. Why? Because the sampling to produce the training and test sets is random. So, while performing the train_test_split, each student built the model based on a somewhat different training set. This means that each student's model is also slightly different from that of her peers.\n",
    "\n",
    "So, whose results are the actual, real results, then?\n",
    "\n",
    "The answer is: We really can't tell. Each tree that each student built has some validity, and we can have some confidence in its final predictions.\n",
    "\n",
    "But wouldn't it be great if we could have more confidence and come to a better overall result for the entire class? That's what the popular Random Forest algorithm does.\n",
    "\n",
    "Random Forest doesn't build just one tree--it builds an entire classroom full of trees, each one of which is based on a slightly different training set (which is, in fact, a small randomized subset of the big overall training set). To save processing power, Random Forest then picks just a random few of the attributes to consider when building each tree, so that no two trees are based on the same attributes. Finally, Random Forest evaluates all the trees it has constructed and, for a given prediction, outputs the class assignment that is the mode of the classes (classification) or, if you run it as a regression tree, the mean prediction (regression) of the individual trees.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/images/randomforest2.png\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "Here is an example:\n",
    "<center>\n",
    "<img src = \"https://static.javatpoint.com/tutorial/machine-learning/images/random-forest-algorithm2.png\">\n",
    "</center>\n",
    "\n",
    "So, we have:\n",
    "* A number of trees\n",
    "* Using a random subset of features in the dataset to make their split decisions\n",
    "* Built on a number of slightly different training subsets, selected as random samples with replacement (= bootstrap aggregating or bagging) from the overall training set\n",
    "* A voting function that selects the mode of the classes (classification or the mean prediction (regression)\n",
    "\n",
    "In other words, we introduce dual randomness into our classification in order to pick the best model from the places where all the individual trees overlap. That leaves us with much greater accuracy for our model.\n",
    "\n",
    "We are working with the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) from the scikit-learn package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8t8Ncq1E8NB"
   },
   "source": [
    "#**0. Preparation and Setup**\n",
    "As before, we are following the basic classification steps:\n",
    "\n",
    "1. **Exploratory Data Analysis** to see how the data is distributed and to determine what the class attribute in the dataset should be. This will be the attribute you'll predict later on\n",
    "2. **Preprocess the data** (remove n/a, transform data types as needed, deal with missing data) and ensure that the dependent attribute is CATEGORICAL\n",
    "3. Split the data into a **training set and a test set**\n",
    "4. **Build the model** based on the training set\n",
    "5. **Test the model** on the test set\n",
    "6. Determine the quality of the model with the help of a **Confusion Matrix** and a **Classification Report**.\n",
    "\n",
    "As with our previous problems, we will use the insurance dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkfMtiHGwLo0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "from IPython.display import HTML # This is just for me so I can embed videos\n",
    "from IPython.display import Image # This is just for me so I can embed images\n",
    "\n",
    "#Reading in the data as insurance dataframe\n",
    "insurance = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/insurance_with_categories.csv\")\n",
    "\n",
    "#Verifying that we can see the data\n",
    "insurance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIgPPPy1HHof"
   },
   "source": [
    "#**1. Exploratory Data Analysis (EDA)**\n",
    "As before, we have the option to either do this in a code cell, or to import the HTML-based ydata_profile package.\n",
    "\n",
    "Test your EDA skills below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryy4rOG4Hpli"
   },
   "outputs": [],
   "source": [
    "insurance.describe(include = 'all'), print(\"***DATA OVERVIEW***\")# Build a data summary for ALL data in the set (not just numeric!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbiFA_LoEned"
   },
   "outputs": [],
   "source": [
    "insurance.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy7vAQWgFDk1"
   },
   "outputs": [],
   "source": [
    "insurance.corr(numeric_only = True), print(\"***DATA CORRELATIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBvEnb0UHyFW"
   },
   "outputs": [],
   "source": [
    "# Build a histogram for the numeric values\n",
    "insurance.hist()\n",
    "insurance.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxHMM_OAH2TL"
   },
   "outputs": [],
   "source": [
    "insurance.groupby('children').size().plot(kind='pie', autopct='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--Jl3_R3IGjl"
   },
   "source": [
    "#**2. Preprocessing: Building the Analysis Set**\n",
    "You have done this before. Build an insurance_rf dataset consisting of age, bmi, children, charges, and--again--region as the class attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7StpkwSIjwk"
   },
   "outputs": [],
   "source": [
    "insurance_rf = pd.DataFrame(insurance, columns = ['age', 'bmi', 'children','charges','region'])\n",
    "insurance_rf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eHI3YdExeDt"
   },
   "source": [
    "# **3. Building the Training and Test Datasets**\n",
    "As before, we cannot do classification without training and test data. You did this previously. Do it again--we want 20% of the data set as test and 80% as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ht6bs3-3xrIN"
   },
   "outputs": [],
   "source": [
    "insurance_rf.children.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bf37-SDIvWuH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=insurance_rf.iloc[:,:4] # all parameters\n",
    "y=insurance_rf['region'] # class labels 'southwest', 'southeast', 'northwest', 'northeast'\n",
    "X_train, X_test, y_train, y_test =  train_test_split(x,y, random_state=0)                            # COMPLETE THIS LINE!\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdH99xuwyoK-"
   },
   "source": [
    "# **4. Building and Training the Classifier**\n",
    "We are going to use the [RandomForestClassifier from sklearn.ensemble](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). The RandomForestClassifier has a number of really interesting parameters that we can control in order to optimize our model to run quickly and efficiently, especially the sub-sample size, which is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DrXRCY0Kpfb"
   },
   "source": [
    "##**4.1 Building the Classifier**\n",
    "\n",
    "The most important parameters are:\n",
    "* n_estimators int, default=100 --\n",
    "The number of trees in the forest.\n",
    "* criterion{“gini”, “entropy”}, default=”gini” --\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific. You've seen this in the previous workbook.\n",
    "* max_features{“auto”, “sqrt”, “log2”}, int or float, default=”auto” --\n",
    "The number of features to consider when looking for the best split: If int, then consider max_features features at each split. If “auto”, then max_features=sqrt(n_features). If “log2”, then max_features=log2(n_features).\n",
    "If None, then max_features=n_features.\n",
    "* max_depthint, default=None --\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "* min_samples_split int or float, default=2\n",
    "The minimum number of samples required to split an internal node\n",
    "* bootstrap bool, default=True -- Whether bootstrap samples are used when building trees (which is 50% of the whole idea behind Random Forest). If False, the whole dataset is used to build each tree.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZPJawY-ykUv"
   },
   "outputs": [],
   "source": [
    "# Configuring the classifier and using get_params to double-check all the parameters with which it is configured\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmzAkEh7dKNM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=insurance_rf.iloc[:,:4] # all parameters\n",
    "y=insurance_rf['region'] # class labels 'southwest', 'southeast', 'northwest', 'northeast'\n",
    "X_train, X_test, y_train, y_test =  train_test_split(x,y, random_state=0)                            # COMPLETE THIS LINE!\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1696135020447,
     "user": {
      "displayName": "Sonja Streuber",
      "userId": "03872648719424952237"
     },
     "user_tz": 240
    },
    "id": "9KO3l6zqcnRQ"
   },
   "outputs": [],
   "source": [
    "rf =RandomForestClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O__ilTPrKzHw"
   },
   "source": [
    "##**4.2 Training the Classifier**\n",
    "Just like before, we are using .fit() to train our classifier! Remember that we named it rf. You'll want your training data inside the parentheses.\n",
    "\n",
    "Give it a shot below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXyCMXBzLLBL"
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geS3TgBALL9z"
   },
   "source": [
    "Just incase you're lost: The solutions are posted at the end of this workbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43eGwtTNQGuQ"
   },
   "source": [
    "# **5. Use the Classifier to test and predict**\n",
    "There is nothing different about the steps below than what you have already done. Uncomment the second line starting with \"print\" if you would like to see the output of your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1696135021381,
     "user": {
      "displayName": "Sonja Streuber",
      "userId": "03872648719424952237"
     },
     "user_tz": 240
    },
    "id": "17nL0-SYQBmC"
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "# print(y_pred) # If you want to see the big long list, uncomment this line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWfQmlvBQoq_"
   },
   "source": [
    "# **6. Evaluate the Quality of the Model**\n",
    "Again, we will look at the following:\n",
    "1. Accuracy score\n",
    "2. Confusion matrix\n",
    "3. Classification Report\n",
    "\n",
    "The interesting part will be to see if any of the predictions have improved from the simple tree model in the previous module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h73LaKzLQzMR"
   },
   "outputs": [],
   "source": [
    "# First, the accuracy score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrLQHFFHNtPT"
   },
   "source": [
    "What was the accuracy score for the simple tree? **Did using Random Forest improve it?** Record your observations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gdah7AcN0x6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjkZplnfRHrP"
   },
   "outputs": [],
   "source": [
    "# Next, the Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=rf.classes_)\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels=rf.classes_).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5VWCR5hOpAk"
   },
   "source": [
    "What did the Confusion Matrix look like for the simple tree? What differences do you notice? Record your observations in the field below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6aYGkqdA2vz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T8LhaHYO0my"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61_SAbqjRosa"
   },
   "outputs": [],
   "source": [
    "# Finally, the Classification Report\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, labels=['southwest', 'southeast', 'northwest', 'northeast']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYp9xLc8PzQg"
   },
   "source": [
    "Again, compare this output with the output for the simple tree. What are the differences? Overall, would you say that Random Forest works better? Or, given that we're doing a whole lot more processing, is any improvement worth it? Record your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AgmJvrhQMhL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJvlXK59SCzn"
   },
   "source": [
    "# **What If ...**\n",
    "So far, we have used only the default settings on the Random Forest algorithm. What if we play with different configuration settings, such as the number of trees? Or the depth of the trees? Or the minimum samples required to split?\n",
    "<center>\n",
    "<img src = \"https://uploads-ssl.webflow.com/646218c67da47160c64a84d5/64634977cc057db29263d4a6_81.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLWRE7YTQ2g8"
   },
   "source": [
    "First, let's set up the parameters as variables so that we can easily change them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1075,
     "status": "ok",
     "timestamp": 1696135127279,
     "user": {
      "displayName": "Sonja Streuber",
      "userId": "03872648719424952237"
     },
     "user_tz": 240
    },
    "id": "yHWW2X3PSXpk"
   },
   "outputs": [],
   "source": [
    "# We are setting up the n_estimators and other configuration parameters so that we can easily change them\n",
    "# Feel free to comment any of these out or change the values and re-run the cells below to see how this changes the result\n",
    "n_estimators = 10000 # This is the number of different trees to build; default was 100; we are increasing this number tenfold.\n",
    "min_samples_split = 5 # Previously, we ran this with the default split of 2\n",
    "criterion='entropy' # This is for Information Gain; previously, we ran this with the Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX24q5UgRBtF"
   },
   "source": [
    "Then, let's build the classifier again, now with the different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPzlGhOxRHGc"
   },
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(verbose=1, n_estimators=n_estimators, min_samples_split=min_samples_split, criterion=criterion)\n",
    "rf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOkCsPkSS3QZ"
   },
   "source": [
    "Time to predict and evaluate with **accuracy score, Confusion Matrix, and Classification Report!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3IlVkRHT8y8"
   },
   "outputs": [],
   "source": [
    "# Testing and predicting\n",
    "y_pred = rf2.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "cm2 = confusion_matrix(y_test, y_pred, labels=rf2.classes_)\n",
    "cm2_display = ConfusionMatrixDisplay(cm, display_labels=rf2.classes_).plot()\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, labels=['southwest', 'southeast', 'northwest', 'northeast']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoLq379zTCpg"
   },
   "source": [
    "##Your Turn\n",
    "**Try this with a number of different settings. Does using 1,000 trees improve the accuracy by a little--or a lot? What about 10,000 trees?**\n",
    "\n",
    "Play around with the settings, then record below what you have done and what your results were. Interpret what you're seeing: Is more processing worth it? Or is there a point where we accept the results as \"good enough\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pS-kgQ8TgvS"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIDox2nGTiPY"
   },
   "source": [
    "#**7. Towards Optimization**\n",
    "<center>\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:640/format:webp/1*lIaapnR-0Cdf3kHsZtGcxw.jpeg\">\n",
    "</center>\n",
    "\n",
    "You just played with the tree setting manually. What if we could cycle the algorithm through a list of \"number of trees\" settings and see what happens then?\n",
    "\n",
    "All we need is a quick \"for\" loop with a range. This range setting is configured like this:\n",
    "\n",
    "\n",
    "```\n",
    "range(starting_point, termination_point, increment_size)\n",
    "```\n",
    "In other words range(20,200,20) means that we start with 20 trees and go up to 200 in steps of 20. So, we will look at the behavior for 20, 40, 60, 80, 100, 120, 140, 160, 180, and 200 trees. The code is below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4ATX_CUVMxt"
   },
   "outputs": [],
   "source": [
    "# We can even cycle through a number of trees in the Random Forest\n",
    "for n_estimators in range(20,200,20):\n",
    "    print('Accuracy score using n_estimators =', n_estimators, end = ': ')\n",
    "\n",
    "    rf3 = RandomForestClassifier(n_estimators = n_estimators, verbose=1)\n",
    "    rf3.fit(X_train, y_train)\n",
    "    y_pred = rf3.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYUPerq3WLg7"
   },
   "source": [
    "How about setting this to 1,000 or even 10,000 trees and seeing the accuracy score change? **Go ahead and play with the range setting!** Then record below which range setting you think works best for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2JHVonXVE6L"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wFvxdHwIm-h"
   },
   "source": [
    "#Solutions (to help you if you get stuck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcGZVdvaItQ2"
   },
   "outputs": [],
   "source": [
    "# This is the solution for task 2 above.\n",
    "insurance_rf = pd.DataFrame(insurance, columns = ['age', 'bmi', 'children','charges','region'])\n",
    "insurance_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBZX_ZvfJYRo"
   },
   "outputs": [],
   "source": [
    "# This is the solution for task 3 above:\n",
    "from sklearn.model_selection import train_test_split\n",
    "x=insurance_rf.iloc[:,:4] # all parameters\n",
    "y=insurance_rf['region'] # class labels 'southwest', 'southeast', 'northwest', 'northeast'\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ut7PtU_dOPPO"
   },
   "outputs": [],
   "source": [
    "# Solution for task 4.2\n",
    "rf.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1rqhokfu8qsmpqNzuK_6maHXAqIwTqzgn",
     "timestamp": 1696134477354
    },
    {
     "file_id": "1eGwNx47F6GIODQPaeHD-1x30P-PH6tSr",
     "timestamp": 1633638839049
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
