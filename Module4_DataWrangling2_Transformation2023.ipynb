{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mh679lW5v_t"
   },
   "source": [
    "#**Module 4: Data Wrangling Part 2**\n",
    "In this module, you will learn how to\n",
    "* Transform data\n",
    "* De-duplicate data\n",
    "* Substitute data\n",
    "* Discretize data\n",
    "\n",
    "**Be sure to expand all the hidden cells, run all the code, and do all the exercises--you will need the techniques for the lesson lab!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-M0cWUsbPO2"
   },
   "source": [
    "#**What is Data Transformation?**\n",
    "\n",
    "Watch the video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1693758489457,
     "user": {
      "displayName": "Sonja Streuber",
      "userId": "03872648719424952237"
     },
     "user_tz": 240
    },
    "id": "42fkWhqm3whB",
    "outputId": "3987c15a-d976-4f7a-ff2e-f5b7632e42ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kx3sOqW5zj4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kx3sOqW5zj4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbWsQuAY3xQQ"
   },
   "source": [
    "Data Transformation is just like this: We put one type of data into our \"transformation\" hat, like strings or categorical data, do our magic, and out comes something else--often numbers, buckets, or other categories. We don't change the size of the hat while we transform the data, that is, we don't change the internal relationships between the data, but we do change their format or the way they are organized. Most often, we change numeric data to different scales, usually between 0 and 1, which is called \"Normalization.'\n",
    "\n",
    "So, when we wrangle data, we transform and map data from one \"raw\" data format (like string, categorical, or numeric) into another format in order to make it usable for a number of analytical purposes. Other terms used for Data Wrangling are Preprocessing or Data Munging. Whatever the word, the goal is to bend the data to our formatting and subsetting goals so that we can use them with the algorithms and other math that we want to use.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnVNWu8SBjIg"
   },
   "source": [
    "#**0. Preparation and Setup**\n",
    "We are working with our adult dataset again, so we're loading our libraries and our dataset just like last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vui8bv375v_v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Reading in the data as adult dataframe\n",
    "adult = pd.read_csv(\"https://raw.githubusercontent.com/shstreuber/Data-Mining/master/data/adult.data.simplified25.csv\")\n",
    "\n",
    "#Verifying that we can see the data\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQDK8kjYdqCl"
   },
   "source": [
    "And here is the adult_small dataframe that you built in the previous notebook. We will use it for a small, fast, proof-of-concept before we use more resources for the larger computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF80q8WMc7Zh"
   },
   "outputs": [],
   "source": [
    "adult_small=adult.iloc[70:80,]\n",
    "adult_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zymnCB2E-szM"
   },
   "source": [
    "# **1. Data Transformation**\n",
    "As you have seen, numeric datatypes allow you to do much more fun math than string datatypes. You can count them, sum them, average them, boxplot them ... all that, while string datatypes, well they can be counted, so much is true--but that really is it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJQHnuM45wBb"
   },
   "source": [
    "\n",
    "We want to transform a categorical attribute, in this case 'race', into a numeric datatype. The Python format for this operation is `DataFrame.astype(dtype, copy=True, errors='raise')`\n",
    "In our case, this would be something like\n",
    "* `adult_small.race.astype('int32')`\n",
    "* `adult_small.race.astype('category')`\n",
    "* `adult_small.race.astype('category', ordered=True)`\n",
    "\n",
    "Let's get started. First, let's verify our datatype for 'race'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzXWvcb35wBd"
   },
   "outputs": [],
   "source": [
    "adult_small.race.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERq9GIGtcACB"
   },
   "source": [
    "'O' means 'object', which is basically a string value. Let's now transform our 'race' attribute to numeric values. Below are all the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HocgWNiGqcIZ"
   },
   "source": [
    "## **1.1 Adding a column with the data we want to transform**\n",
    "We could theoretically transform the 'race' attribute in place, but if our code causes problems, we could mess up our entire dataframe. So, instead, we will first add a new column into which we will copy the contents of the 'race' attribute. In a second step, we will transform the values in that new column.\n",
    "\n",
    "Detailed explanations are in the code comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zpk1SU9w5wBu"
   },
   "outputs": [],
   "source": [
    "# Creating a new attribute and populating it with the contents of the attribute that we want to transform.\n",
    "race_num=adult_small.race\n",
    "\n",
    "# Adding the new attribute to the dataframe\n",
    "adult_small['race_num'] = race_num\n",
    "\n",
    "# Checking the contents of the new attribute\n",
    "adult_small.race_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c6aiJYUmwOn"
   },
   "outputs": [],
   "source": [
    "# Let's check the shape now. Instead of 12 attributes, we should have 10 rows and 13 columns now because we have built a new race_num attribute\n",
    "adult_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJQe0tGsl7i-"
   },
   "outputs": [],
   "source": [
    "# Now that we have the race_num column built, let's check the datatypes of both attributes.\n",
    "adult_small.dtypes[['race','race_num']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KgdeN0Mqp9j"
   },
   "source": [
    "## **1.2 Transforming the values**\n",
    "To transform an object value to numeric, we need to get the numbers from somewhere. In our case, we will transform the object to category. That gives us the index numbers for the category. Then we will replace the string values with these index numbers for the category--and voila! we have a numeric transformation.\n",
    "(If you see any attribute replacement warnings as you run the code below, you can ignore them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLD7OLlm5wB1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here, we convert the race_num values to categorical (to obtain the index numbers)\n",
    "adult_small['race_num']= adult_small['race_num'].astype('category')\n",
    "\n",
    "# Now we replace the categories with their index numbers, i.e. the category codes\n",
    "adult_small['race_num']= adult_small['race_num'].cat.codes\n",
    "\n",
    "# Let's see what the datatypes look like now\n",
    "adult_small.dtypes[['race','race_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xL7eGHW5wB8"
   },
   "outputs": [],
   "source": [
    "# Now let's display the contents of both attributes to double-check\n",
    "adult_small[['race','race_num']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDA_qvRzsOQX"
   },
   "source": [
    "**SUCCESS!** Now that we have worked the code out for our small proof-of-concept dataframe, we can apply it to the entire adult dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7o-UELnu5wCG"
   },
   "outputs": [],
   "source": [
    "# First, we back the dataframe up.\n",
    "adult2=adult\n",
    "\n",
    "# Now we build the column and copy the original values into it\n",
    "adult2['race_num'] = adult2.race\n",
    "\n",
    "# Third, we convert race_num into categorical\n",
    "adult2['race_num']= adult2['race_num'].astype('category')\n",
    "\n",
    "# Then, we replace the strings with the category indices\n",
    "adult2['race_num']= adult2['race_num'].cat.codes\n",
    "\n",
    "# Now, we check the data types\n",
    "adult2.dtypes[['race','race_num']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGqvx7QtehQ"
   },
   "source": [
    "**YAY!** That worked, too. But how did pandas assign the numbers? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwGxPCDp5wCN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Making a temporary new dataframe because it's so much easier to work with just 2 attributes\n",
    "race_exploration=adult2[['race','race_num']]\n",
    "race2=race_exploration.drop_duplicates()\n",
    "race2.sort_values(by=['race_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRDyehO-n-7g"
   },
   "source": [
    "##**Your Turn**\n",
    "Explain in a couple of complete sentences the logic with which pandas haa ssigned the numbers for the race_num array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfLScvE35wCU"
   },
   "source": [
    "# **2. De-Duplication**\n",
    "Check out this line in the example above:\n",
    "`race2=race_exploration.drop_duplicates()`\n",
    "\n",
    "This is literally all there is to de-duplication.\n",
    "For more on drop_duplicates() see, https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaQm7prI5wCV"
   },
   "source": [
    "# **3. Substituting Values in Dataframes**\n",
    "\n",
    "OK--tiger, meet lion, and lion--meet rhino!\n",
    "\n",
    "<img src=\"https://github.com/shstreuber/Data-Mining/blob/master/images/transformation.jpg?raw=true\">\n",
    "\n",
    "Here is how to substitute values in a dataframe. For this purpose, we will try out the df.replace method shown in https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html. This really convenient method works for strings and numeric values alike.\n",
    "\n",
    "Here, we will work with the workclass attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDfB_pNT5wCW"
   },
   "outputs": [],
   "source": [
    "adult3 = adult\n",
    "adult3.workclass.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnX1383hzPRl"
   },
   "source": [
    "### The Problem\n",
    "Oh, look: We have a '?' value in there. This could be any of the following\n",
    "* Misformatted data that might have accidentally broken during the E of [ETL](https://www.sas.com/en_us/insights/data-management/what-is-etl.html).\n",
    "* Some sort of placeholder character that our data warehouse has inserted for missing values.\n",
    "* Intentional and supposed to stand for 'Unknown'.\n",
    "\n",
    "Whatever it is, we may never know. That is one of the big problems when dealing with big datasets. The next problem is:\n",
    "**bold text**\n",
    "> **What are we going to do with it?**\n",
    "\n",
    "Are we going to just **ignore** the rows in which the '?' appears--or are we going to **do something about it**? To answer that question, it is helpful to know how many rows contain the '?'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epXscXUH5wCd"
   },
   "outputs": [],
   "source": [
    "adult3[adult3['workclass'] == '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8nxCFHy2hBF"
   },
   "source": [
    "Hm. That's 1,399 rows out of 25,000--5.5% of all the data. At this point, it's a good idea to ask your supervisor, that is, the client of your analysis, what they would prefer. Let's say the client comes back to say they **don't** want you to drop 6% of the dataset.\n",
    "\n",
    "At that point, you will have to make an assumption--and that's where critical thinking skills come in: Does it matter whether there's an accidental or an intentional '?'. If it does, then further research is needed. If it does NOT, then it might be safe to assume that the '?' stands for 'Unknown'.\n",
    "\n",
    "That's why we will translate '?' to 'Unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cg9UwBDC5wCj"
   },
   "outputs": [],
   "source": [
    "# Instead of building another column to see any transformations, we will substitute data in place this time around.\n",
    "# For this purpose, we're using the replace() method from pandas\n",
    "adult3.workclass.replace('?','Unknown', inplace=True)\n",
    "\n",
    "# Did we indeed replace all '?'? Let's check:\n",
    "adult3[adult3['workclass'] == '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORwHoBmN5wCo"
   },
   "outputs": [],
   "source": [
    "# Looks like it worked. Let's check if the new value appears in the list of unique values.\n",
    "adult3.workclass.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn3ggAxc4mnJ"
   },
   "source": [
    "## **Your Turn**\n",
    "Take a look at the unique values in the occupation attribute. Do you see something similar as what we solved above?\n",
    "1. Replace the unusual value with 'NA'\n",
    "2. Then count how many rows contain 'NA' values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgd8uiXw_iS1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNMl3Ud_5wCz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49JiTxR35wDy"
   },
   "source": [
    "# **4. Discretization/ Aggregation**\n",
    "Remember how I said we're also going to exchange the lion and the tiger from the Rocky and Bullwinkle cartoon into a \"Big Cats\" category? That's essentially what Discretization is about.\n",
    "\n",
    "<img src = \"https://github.com/shstreuber/Data-Mining/blob/master/images/aggregation.jpg?raw=true\">\n",
    "\n",
    "Data discretization is typically defined as a process of converting continuous data attribute values into a finite set of intervals and associating with each interval some specific data value. So, you could say that the goal of discretization is to simplify data by putting them into summary categories. While discretization is typically applied to numeric data only, we can also use this concept to simplify other data (like object-type data or data that has too many categories to be useful).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryIUaek080A5"
   },
   "source": [
    "##**4.1 Data Summarization**\n",
    "We're not done with the workclass attribute just yet. As you have seen, we have 9 categories. We will reduce these to the following:\n",
    "- Unknown--contains all Unknown, Never-worked, and Without-pay\n",
    "- Government--contains all Federal, State, and Local government employees\n",
    "- Self--contains all self-employed groups\n",
    "\n",
    "This summarization will allow us to more easily group our data by the three larger units in the workclass attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RmNwoXC5wDz"
   },
   "outputs": [],
   "source": [
    "# Let's check the categories in the workclass attribute!\n",
    "adult3.workclass.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d62mRQex5wD_"
   },
   "source": [
    "Given the structure of this attribute, we are seeing three \"natural\" groups: Unknown, Government, and self-employed people. This means we can simplify the data we have and create three big aggregations with simple substitutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1693760532856,
     "user": {
      "displayName": "Sonja Streuber",
      "userId": "03872648719424952237"
     },
     "user_tz": 240
    },
    "id": "zyEPwKqY5wEA"
   },
   "outputs": [],
   "source": [
    "#Summarizing all the Unknowns\n",
    "adult3.workclass.replace('Unknown','Unknown', inplace=True)\n",
    "adult3.workclass.replace('Never-worked','Unknown', inplace=True)\n",
    "adult3.workclass.replace('Without-pay','Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1693760548444,
     "user": {
      "displayName": "Sonja Streuber",
      "userId": "03872648719424952237"
     },
     "user_tz": 240
    },
    "id": "xqvKM57l8yxt"
   },
   "outputs": [],
   "source": [
    "#Summarizing all the Government attributes\n",
    "adult3.workclass.replace('State-gov', 'Government', inplace=True)\n",
    "adult3.workclass.replace('Federal-gov', 'Government', inplace=True)\n",
    "adult3.workclass.replace('Local-gov', 'Government', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTGaIHBom8O2"
   },
   "source": [
    "##**Your Turn**\n",
    "Complete the job! Summarize the other three groups into the \"Self\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODdtCWvLnq6f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDpoPQdC7Q6C"
   },
   "source": [
    "##**4.2 Binning and Bucketing (Numeric Discretization)**\n",
    "We have arrived at the \"pure\" meaning of discretization--the one you find in most textbooks. In this context, you'll often hear the terms binning and bucketing.\n",
    "\n",
    "In short, Data binning and bucketing groups data in bins or buckets, in the sense that it replaces values contained into a small interval with a single representative value for that interval. It includes also dealing with missing values, formatting, normalization and standardization. That makes it a technique for data smoothing.\n",
    "\n",
    "The **difference** between binning and bucketing is:\n",
    "* In **bucketing**, we define the boundaries for the categories ourselves. They don't have to be the same size.\n",
    "* In **binning**, we define the number of categories, and the code distributes them evenly across our dataset, depending on whether we set the binning criterion to the number of data in each bin, or to the value of the data.\n",
    "\n",
    "On a technical level, we to convert numeric values to categorical or to sample (quantise) numeric values, sometimes using distance (remember distance from one of the previous modules?) and sometimes using frequency. we can also reduce numeric values through sampling.\n",
    "\n",
    "Normalization and standardization are special cases of Discretization that we'll talk about in a bit. To learn more in depth about binning, go [here](https://towardsdatascience.com/data-preprocessing-with-python-pandas-part-5-binning-c5bd5fd1b950). To find out more about how to do this at a practical level, take a look at [this explanation](https://stackoverflow.com/questions/45273731/binning-column-with-python-pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxmiSUvP_39G"
   },
   "source": [
    "###**Binning**\n",
    "It's time to look at incomeUSD. To remind yourself what the attribute looks like, use the describe() function to produce a 5-number summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1693760601624,
     "user": {
      "displayName": "Sonja Streuber",
      "userId": "03872648719424952237"
     },
     "user_tz": 240
    },
    "id": "t8P0HrN3ANfx",
    "outputId": "2ee7c1b5-9106-40bf-d7ba-68297c394d51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     25000.000000\n",
       "mean      56382.802560\n",
       "std       44128.661114\n",
       "min       20002.000000\n",
       "25%       29963.750000\n",
       "50%       39689.000000\n",
       "75%       49588.500000\n",
       "max      199960.000000\n",
       "Name: incomeUSD, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.incomeUSD.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ks-OBjCVDnsK"
   },
   "source": [
    "We see that the income ranges from 20,002 to 199,960. How about making three categories: Low, medium, and high? If we are building 3 groups, we need 4 edges of intervals (**BINS**):\n",
    "\n",
    "\n",
    "<img src =\"https://github.com/shstreuber/Data-Mining/blob/master/images/3bins.jpg?raw=true\">\n",
    "\n",
    "* small — (edge1, edge2)\n",
    "* medium — (edge2, edge3)\n",
    "* high — (edge3, edge4)\n",
    "\n",
    "We can use the linspace() function of the numpy package to calculate the 4 bins, equally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T64NDhl2ENpf"
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(adult.incomeUSD.min(),adult.incomeUSD.max(),4)\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do8EHOXaHgzB"
   },
   "source": [
    "Pandas has calculated the following cutoffs:\n",
    "* Group 1: 20,002.00000 to 79,988\n",
    "* Group 2: 79,988 to 139,974\n",
    "* Group 3: 139,974 to 199,960\n",
    "\n",
    "Now we define the three labels (small, medium, high) for these three groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVpVkXFiHtCJ"
   },
   "outputs": [],
   "source": [
    "labels = ['small', 'medium', 'high']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7ca50_SH2oI"
   },
   "source": [
    "We can use the cut() function to convert the numeric values of incomeUSD into the categorical values of small, medium, and high. We need to specify the bins and the labels. In addition, we set the parameter include_lowest to True in order to include also the minimum value. In order not to change our original dataset, we'll create an adult3 backup dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2wS6iYGISuO"
   },
   "outputs": [],
   "source": [
    "adult3=adult\n",
    "adult3['bins'] = pd.cut(adult3['incomeUSD'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpftY9yYJKae"
   },
   "source": [
    "This gives us the new 'bins' attribute at the end of our adult3 dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QI3XNXFBKS8-"
   },
   "outputs": [],
   "source": [
    "adult3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGp6hc_2KeKe"
   },
   "source": [
    "We can now use groupby() on this new 'bins' attribute to count the data in each of these categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PL_FSOjJWIk"
   },
   "outputs": [],
   "source": [
    "adult3.groupby(['bins']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3cyUciNL7nn"
   },
   "source": [
    "We can even plot them in a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgJt0fxbL-Us"
   },
   "outputs": [],
   "source": [
    "plt.hist(adult3['bins'], bins=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbXIIiasMKPb"
   },
   "source": [
    "Let's put our **CRITICAL THINKING** hats on: The cutoffs for the bins that we have calculated were:\n",
    "* Group 1: 20,002.00000 to 79,988\n",
    "* Group 2: 79,988 to 139,974\n",
    "* Group 3: 139,974 to 199,960\n",
    "\n",
    "What we are seeing in the histogram is that a huge majority of our datapoints falls into the \"small\" category, so below 79,988, whereas a much smaller number falls into the medium and high categories.\n",
    "\n",
    "Are these bins useful? Not really. The only thing they show us is that we have a whole lot of poor people in our dataset. Let's go back to the 5-number summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H-OPphgNjLf"
   },
   "outputs": [],
   "source": [
    "adult3.incomeUSD.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7L7lvgmAP4O"
   },
   "source": [
    "###**Bucketing**\n",
    "We see that the income ranges from 20,002 to 199,960, with a median at $39,689. That's not even a fifth of the top salary! Such a low median means that 50% of the people in our dataset make less than 39,732 while the remaining 50% inhabit the range from 39,689 to 199,960. In fact, only 25% of people earn between 49,588 (Q3) and the top salary. Wouldn't it be interesting to see how many earners we have in the top range, between, say 150,000 and 199,000?\n",
    "\n",
    "That's where **BUCKETS** come in.\n",
    "\n",
    "With buckets, you set the interval cutoffs manually. So, how about we make 5 buckets:\n",
    "* poor (20002 to the median of 39689)\n",
    "* low-wage-earners (median of 39689 to Q3 aka 49588)\n",
    "* middle-class (49588 to 79988 (which was the previous Group 2 cutoff))\n",
    "* rich (79988 to 150000)\n",
    "* super-rich (150000 to 199960)\n",
    "\n",
    "<img src = \"https://github.com/shstreuber/Data-Mining/blob/master/images/5buckets_named_diff.jpg?raw=true\">\n",
    "\n",
    "You're probably noticing that these buckets aren't the same size. With bucketing, that's not the point. We get to make the cutoffs.\n",
    "\n",
    "Now, let's code this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8M4MNCv_PwHC"
   },
   "outputs": [],
   "source": [
    "# Setting up our 5 buckets and their labels\n",
    "buckets = [ 20002, 39689, 49588, 79988, 150000, 199960 ]\n",
    "bucketlabels = ['poor', 'low-wage-earners', 'middle-class', 'rich', 'super-rich']\n",
    "\n",
    "# Using cut() to separate data into the buckets we have built\n",
    "adult3['buckets'] = pd.cut(adult3['incomeUSD'] , bins=buckets, labels=bucketlabels, include_lowest=True)\n",
    "\n",
    "# Check if we have the new attribute 'buckets' at the end of our dataset\n",
    "adult3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrrOUHnJRpzv"
   },
   "outputs": [],
   "source": [
    "# Checking the contents of our buckets\n",
    "adult3.groupby(['buckets']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0gv6VPHRY2y"
   },
   "outputs": [],
   "source": [
    "# Plotting the distribution now\n",
    "plt.hist(adult3['buckets'], bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YopuAWo7TPuw"
   },
   "source": [
    "###**CRITICAL THINKING MOMENT**\n",
    "As you can see, bucketing gives us much better information about our dataset, but we needed to use binning to point us in the right direction! Now, look at these three bucket cutoffs again:\n",
    "\n",
    "* poor (20002 to the median of 39689)\n",
    "* rich (79988 to 150000)\n",
    "* super-rich (150000 to 199960)\n",
    "\n",
    "We are seeing that the number of poor people is still vastly bigger than the numbers of rich and super-rich people. **AND** we see that most of the poverty is concentrated between 20002 and 39689--that's a range of less than 20000--whereas the range of rich to super-rich income is much larger, from 79988 to 199960--that's roughly 120000!\n",
    "<center>\n",
    "<img src = \"https://thumbs.dreamstime.com/z/mind-blown-emoji-exploding-head-emoticon-white-background-d-rendering-219279605.jpg?w=300\">\n",
    "</center>\n",
    "\n",
    "**DISCLAIMER**: Of course, we are evaluating the income from an American perspective, even though the numbers come from countries all over the world, and we are not looking at buying power by country, at all. For example, $39,689 buys much more in a country like Poland than it does in the US. So, to get more balanced insights, we would have to do much more investigating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6xIuxsFWacO"
   },
   "source": [
    "##**4.3 Normalization and Standardization**\n",
    "A third type of Discretization, which will become critical when we talk about Neural Networks, helps us compare the values inside numeric attributes in a dataset by putting them all on the same scale.\n",
    "\n",
    "Below, I will show you the most popular method for normalization, with the MixMaxScaler from the new-to-you scikit-learn package, which transforms values to a scale from 0 to 1. Then, I will show you two methods for standardization: A basic pandas method and the StandardScaler from the--you guessed it--scikit-learn package\n",
    "\n",
    "**CRITICAL THINKING MOMENT**\n",
    "Which data would we want to compare? Typically the ones that can have meaning for one another, like the ones with the same units. So here, we will use age and educationyears to normalize because both are in the same unit: Years.\n",
    "\n",
    "First, we will pull out these two numeric attributes into their own dataframe. That will make our processing faster and easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqV3dLuQhMXK"
   },
   "outputs": [],
   "source": [
    "adult_years = adult.iloc[:,[0,3]]\n",
    "adult_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrzeVmNyj7LH"
   },
   "outputs": [],
   "source": [
    "# Better safe than sorry! Let's check the dataypes.\n",
    "adult_years.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyk05eojo0V0"
   },
   "source": [
    "###**Normalization with scikit-learn**\n",
    "Scikit-learn is a scientific package that allows us to do biased estimating, based on how you tell the algorithm to calculate the normalization range. Some data scientists prefer the MixMax method over the pandas method because it gives more control over how to scale the attributes. The most popular approach is [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), which scales and translates each attribute individually such that it falls between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yN5my7TOqdkl"
   },
   "outputs": [],
   "source": [
    "# Normalization with MinMaxScaler\n",
    "\n",
    "from sklearn import preprocessing # the package\n",
    "\n",
    "adult_years1 = adult_years # backing up my dataframe--you never know!\n",
    "x = adult_years1.values # pulling out just the array values\n",
    "\n",
    "# Now we are using the scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Check the link above for more explanation of this line\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "adult_years1 = pd.DataFrame(x_scaled)\n",
    "adult_years1 # We should now be seeing all values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89rOe8CZuc0u"
   },
   "source": [
    "###**Standardization with scikit-learn**\n",
    "Standardization is typically when values are organized around the mean and divided by the standard deviation. The fastest approach here is the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXaxU2zLtaAm"
   },
   "outputs": [],
   "source": [
    "# Standardization with StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # the package\n",
    "\n",
    "adult_years2 = adult_years # backing up my dataframe--you never know!\n",
    "scaler = StandardScaler() # setting an abbreviation to make things easier\n",
    "\n",
    "#adult_years2 = scaler.fit_transform(adult_years2.to_numpy()) # if you want to add an extra transformation to a numpy array\n",
    "adult_years2 = scaler.fit_transform(adult_years2)\n",
    "\n",
    "adult_years2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJZWOZTkfyma"
   },
   "source": [
    "### **Standardization with pandas**\n",
    "Using pandas for normalization is pretty straightforward: From each data value in an attribute, we subtract the mean and divide by the standard deviation. We do this row by row, so we specify axis=0. (axis=1 means column, remember?).\n",
    "\n",
    "Cycling through our data will require the lambda transformation we have seen in a previous module. To learn more about this lambda transformation, check [this link](https://www.analyticsvidhya.com/blog/2020/03/what-are-lambda-functions-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DXl7KEygOh5"
   },
   "outputs": [],
   "source": [
    "adult_years3 = adult_years.apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "adult_years3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HiBqlMHyAPf"
   },
   "source": [
    "**OH! WAIT! WHAT!!!!???**\n",
    "\n",
    "Compare these values to the output for the StandardScaler--do you notice anything?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTOebMxpnRac"
   },
   "source": [
    "### **The Big Picture**\n",
    "If you don't want to build a separate dataframe, that's ok, but it takes up more processing power. You will have to work with iloc to tell Python which columns to transform (I am using adult3 below with the pandas transformation just because we did that last and because I don't want to break the original adult dataframe here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8j5ifKNni2K"
   },
   "outputs": [],
   "source": [
    "adult3.iloc[:,[0,3]] = adult.iloc[:,[0,3]].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "adult3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6svM7QYuEGSd"
   },
   "source": [
    "##**Your Turn**\n",
    "Now apply the MinMaxScaler to the age and educationyears attribute throughout the ENTIRE adult dataset (back it up as adult4 so you can always quickly replace it). For this, you will combine code from the MinMaxScaler normalization with the code directly above that applies the transformation to specific columns/ attributes/ dimensions/ features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBa1eNNMEzsX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "13gMg_WLEhNpJ4s16i5YH0mBYDHfX7ocn",
     "timestamp": 1622296550607
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
